{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Reading From Dataset\n",
    "First and foremost, we'll need a dataset to train your model because it's virtually impossible to complete a machine-learning project without one, right? :) We can easily read the dataset thanks to [Hossein Zaredar](https://github.com/HosseinZaredar/Computational-Intelligence/blob/main/read_MNIST.py)'s help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NUMBER_OF_PIXELS = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To read from the files, we construct a function named `read_from_file` that does nothing but that.We know where to look for each data based on the information given by MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_file(image_address, label_address):\n",
    "    images_file = open(image_address, 'rb')\n",
    "    images_file.seek(4)  # Positions the cursor to the 4th byte.\n",
    "    number_of_images = int.from_bytes(images_file.read(4), 'big')  # reads 4 bytes in big endian order\n",
    "    images_file.seek(16)  # Positions the cursor to the 16th byte.\n",
    "    \n",
    "    labels_file = open(label_address, 'rb')\n",
    "    labels_file.seek(8)\n",
    "    \n",
    "    result_set = []\n",
    "    for n in range(number_of_images):\n",
    "        image = np.zeros((NUMBER_OF_PIXELS, 1))\n",
    "        for i in range(NUMBER_OF_PIXELS):\n",
    "            image[i, 0] = int.from_bytes(images_file.read(1), 'big') / 255\n",
    "\n",
    "        label_value = int.from_bytes(labels_file.read(1), 'big')\n",
    "        label = np.zeros((10, 1))  # Since we have 10 numbers from 0 to 9\n",
    "        label[label_value, 0] = 1\n",
    "\n",
    "        result_set.append((image, label))\n",
    "    \n",
    "    return result_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we call that function to create our train & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = read_from_file(\"train-images.idx3-ubyte\", \"train-labels.idx1-ubyte\")\n",
    "test_set = read_from_file(\"t10k-images.idx3-ubyte\", \"t10k-labels.idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of this section, we plot an image just to make sure we've done this part rightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_set[0][0].reshape(28, -1), 'gray')\n",
    "train_set[0][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feedforward\n",
    "As we all know, in order to calculate output in a neural network based on inputs, we must apply the following formula on each layer:  \n",
    "$$a^{(L+1)} = \\sigma(W^{(L+1)}× a^{(L)} + b^{(L+1)})$$\n",
    "\n",
    "Therefore in implementation, for weights between layers, we assign a k×n matrix. Assume \"k\" is the number of neurons on the next layer, and \"n\" is the number of neurons on the current layer. As a result, the weights of a single neuron on the next layer are shown in each row of our matrix W."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate W matrix and vector b for each layer.\n",
    "\n",
    "# Initialize W from standard normal distribution\n",
    "W1 = np.random.normal(size=(16, NUMBER_OF_PIXELS))\n",
    "W2 = np.random.normal(size=(16, 16))\n",
    "W3 = np.random.normal(size=(10, 16))\n",
    "\n",
    "# Initialize b = 0, for each layer.\n",
    "b1 = np.zeros((16, 1))\n",
    "b2 = np.zeros((16, 1))\n",
    "b3 = np.zeros((10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step of this project, after initializing W matrices and bias vectors, we separate the first 100 images of our train dataset and calculate the output of that based on the given formula.\n",
    "\n",
    "In the end, we report the accuracy, which is the number of true estimations divided by the number of images (which is 100 in our scenario). Regarding that the learning process has not proceeded, we expect to have an accuracy of around 10%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.04\n"
     ]
    }
   ],
   "source": [
    "number_of_correct_estimations = 0\n",
    "total_numbers = 100\n",
    "\n",
    "for train_data in train_set[:total_numbers]:\n",
    "    a0 = train_data[0]\n",
    "    a1 = sigmoid(W1 @ a0 + b1)\n",
    "    a2 = sigmoid(W2 @ a1 + b2)\n",
    "    a3 = sigmoid(W3 @ a2 + b3)\n",
    "    \n",
    "    predicted_number = np.where(a3 == np.amax(a3))\n",
    "    real_number = np.where(train_data[1] == np.amax(train_data[1]))\n",
    "    \n",
    "    if predicted_number == real_number:\n",
    "        number_of_correct_estimations += 1\n",
    "\n",
    "print(f\"Accuracy: {number_of_correct_estimations / total_numbers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backpropagation\n",
    "![Layers](assets/Layers.jpg)\n",
    "As we know, the learning process in a neural network is equivalent to minimize the cost function  \n",
    "$$Cost =\\sum_{j=0}^{n_{L} - 1} (a_{j}^{(L)} - y_{j})^2$$\n",
    "That is done with the help of Gradient Descent. To do that, we take a partial derivative of the cost function with respect to all the parameters to make the Gradient vector.\n",
    "$$(W, b) = (W, b) - \\alpha\\nabla Cost$$\n",
    "We take the derivatives with the help of backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 10\n",
    "learning_rate = 1\n",
    "number_of_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use backpropagation?\n",
    "Regarding we have four layers (one input layer, one output layer, and two hidden layers), if we want to use backpropagation, at first, we have to define what we are dealing with.  \n",
    "Let us suppose we name our layers from 0 to 3, then we have:\n",
    "$$Cost =\\sum_{j=0}^{9} (a_{j}^{(L)} - y_{j})^2$$\n",
    "Each neuron at the last layer is equal to:\n",
    "$$a_{j}^{(3)} = \\sigma(z_{j}^{(3)})$$\n",
    "And $z_{j}^{(3)}$ is equal to:\n",
    "$$z_{j}^{(3)} = \\sum_{j=0}^{9} w_{jk}^{(3)}a_{k}^{(2)} + b_{j}^{(2)}$$\n",
    "### The last layer\n",
    "##### Weight\n",
    "If we apply the chain rule, we can reach to the following formula:  \n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial w_{jk}^{(3)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{j}^{(3)}} × \\displaystyle \\frac{\\partial a_{j}^{(3)}}{\\partial z_{j}^{(3)}} × \\displaystyle \\frac{\\partial z_{j}^{(3)}}{\\partial w_{jk}^{(3)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial w_{jk}^{(3)}} = 2(a_{j}^{(3)} - y_{j}) × \\sigma^{'}(z_{j}^{(3)})×a_{k}^{(2)}$$\n",
    "\n",
    "##### Bias\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial b_{j}^{(3)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{j}^{(3)}} × \\displaystyle \\frac{\\partial a_{j}^{(3)}}{\\partial z_{j}^{(3)}} × \\displaystyle \\frac{\\partial z_{j}^{(3)}}{\\partial b_{j}^{(3)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial b_{j}^{(3)}} = 2(a_{j}^{(3)} - y_{j}) × \\sigma^{'}(z_{j}^{(3)})× 1$$\n",
    "\n",
    "##### Activation\n",
    "We also need to calculate partial derivatives with respect to the activation output of the previous layer. It helps us for backpropagation as we see further.  \n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} = \\sum_{j=0}^{9} \\displaystyle \\frac{\\partial Cost}{\\partial a_{j}^{(3)}} × \\displaystyle \\frac{\\partial a_{j}^{(3)}}{\\partial z_{j}^{(3)}} × \\displaystyle \\frac{\\partial z_{j}^{(3)}}{\\partial a_{k}^{(2)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} = \\sum_{j=0}^{9} (2(a_{j}^{(3)} - y_{j}) × \\sigma^{'}(z_{j}^{(3)})× w_{jk}^{(3)}) $$\n",
    "\n",
    "### 3rd layer\n",
    "##### Weight\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial w_{km}^{(2)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} × \\displaystyle \\frac{\\partial a_{k}^{(2)}}{\\partial z_{k}^{(2)}} × \\displaystyle \\frac{\\partial z_{k}^{(2)}}{\\partial w_{km}^{(2)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial w_{km}^{(2)}} =  \\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} × \\sigma^{'}(z_{k}^{(2)})×a_{m}^{(1)}$$\n",
    "\n",
    "##### Bias\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial b_{k}^{(2)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} × \\displaystyle \\frac{\\partial a_{k}^{(2)}}{\\partial z_{k}^{(2)}} × \\displaystyle \\frac{\\partial z_{k}^{(2)}}{\\partial b_{k}^{(2)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial b_{k}^{(2)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} × \\sigma^{'}(z_{k}^{(2)})× 1$$\n",
    "\n",
    "##### Activation\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial a_{m}^{(1)}} = \\sum_{k=0}^{15} \\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} × \\displaystyle \\frac{\\partial a_{k}^{(2)}}{\\partial z_{k}^{(2)}} × \\displaystyle \\frac{\\partial z_{k}^{(2)}}{\\partial a_{m}^{(1)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial a_{m}^{(1)}} = \\sum_{k=0}^{15} (\\displaystyle \\frac{\\partial Cost}{\\partial a_{k}^{(2)}} × \\sigma^{'}(z_{k}^{(2)})× w_{km}^{(2)}) $$\n",
    "\n",
    "### 2nd layer\n",
    "##### Weight\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial w_{mv}^{(1)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{m}^{(1)}} × \\displaystyle \\frac{\\partial a_{m}^{(1)}}{\\partial z_{m}^{(1)}} × \\displaystyle \\frac{\\partial z_{m}^{(1)}}{\\partial w_{mv}^{(1)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial w_{mv}^{(1)}} =  \\displaystyle \\frac{\\partial Cost}{\\partial a_{m}^{(1)}} × \\sigma^{'}(z_{m}^{(1)})×a_{v}^{(0)}$$\n",
    "\n",
    "##### Bias\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial b_{m}^{(1)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{m}^{(1)}} × \\displaystyle \\frac{\\partial a_{m}^{(1)}}{\\partial z_{m}^{(1)}} × \\displaystyle \\frac{\\partial z_{m}^{(1)}}{\\partial b_{m}^{(1)}} $$\n",
    "\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial b_{m}^{(1)}} = \\displaystyle \\frac{\\partial Cost}{\\partial a_{m}^{(1)}} × \\sigma^{'}(z_{m}^{(1)})× 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total_costs = []\n",
    "# Initialize W with random normal distribution for each layer.\n",
    "W1 = np.random.normal(size=(16, NUMBER_OF_PIXELS))\n",
    "W2 = np.random.normal(size=(16, 16))\n",
    "W3 = np.random.normal(size=(10, 16))\n",
    "\n",
    "# Initialize b = 0, for each layer.\n",
    "b1 = np.zeros((16, 1))\n",
    "b2 = np.zeros((16, 1))\n",
    "b3 = np.zeros((10, 1))\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    batches = [train_set[x:x+batch_size] for x in range(0, 100, batch_size)]\n",
    "    for batch in batches:\n",
    "        # allocate grad_W matrix for each layer\n",
    "        grad_W1 = np.zeros((16, NUMBER_OF_PIXELS))\n",
    "        grad_W2 = np.zeros((16, 16))\n",
    "        grad_W3 = np.zeros((10, 16))\n",
    "        # allocate grad_b for each layer\n",
    "        grad_b1 = np.zeros((16, 1))\n",
    "        grad_b2 = np.zeros((16, 1))\n",
    "        grad_b3 = np.zeros((10, 1))\n",
    "        \n",
    "        for image, label in batch:\n",
    "            # compute the output (image is equal to a0)\n",
    "            a1 = sigmoid(W1 @ image + b1)\n",
    "            a2 = sigmoid(W2 @ a1 + b2)\n",
    "            a3 = sigmoid(W3 @ a2 + b3)\n",
    "            \n",
    "            # ---- Last layer\n",
    "            # weight\n",
    "            for j in range(grad_W3.shape[0]):\n",
    "                for k in range(grad_W3.shape[1]):\n",
    "                    grad_W3[j, k] += 2 * (a3[j, 0] - label[j, 0]) * a3[j, 0] * (1 - a3[j, 0]) * a2[k, 0]\n",
    "            \n",
    "            # bias\n",
    "            for j in range(grad_b3.shape[0]):\n",
    "                    grad_b3[j, 0] += 2 * (a3[j, 0] - label[j, 0]) * a3[j, 0] * (1 - a3[j, 0])\n",
    "            \n",
    "            # ---- 3rd layer\n",
    "            # activation\n",
    "            delta_3 = np.zeros((16, 1))\n",
    "            for k in range(16):\n",
    "                for j in range(10):\n",
    "                    delta_3[k, 0] += 2 * (a3[j, 0] - label[j, 0]) * a3[j, 0] * (1 - a3[j, 0]) * W3[j, k]\n",
    "            \n",
    "            # weight\n",
    "            for k in range(grad_W2.shape[0]):\n",
    "                for m in range(grad_W2.shape[1]):\n",
    "                    grad_W2[k, m] += delta_3[k, 0] * a2[k,0] * (1 - a2[k, 0]) * a1[m, 0]\n",
    "            \n",
    "            # bias\n",
    "            for k in range(grad_b2.shape[0]):\n",
    "                    grad_b2[k, 0] += delta_3[k, 0] * a2[k, 0] * (1 - a2[k, 0])\n",
    "                    \n",
    "            # ---- 2nd layer\n",
    "            # activation\n",
    "            delta_2 = np.zeros((16, 1))\n",
    "            for m in range(16):\n",
    "                for k in range(16):\n",
    "                    delta_2[m, 0] += delta_3[k, 0] * a2[k, 0] * (1 - a2[k, 0]) * W2[k, m]\n",
    "            \n",
    "            # weight\n",
    "            for m in range(grad_W1.shape[0]):\n",
    "                for v in range(grad_W1.shape[1]):\n",
    "                    grad_W1[m, v] += delta_2[m, 0] * a1[m,0] * (1 - a1[m, 0]) * image[v, 0]\n",
    "                    \n",
    "            # bias\n",
    "            for m in range(grad_b1.shape[0]):\n",
    "                    grad_b1[m, 0] += delta_2[m, 0] * a1[m, 0] * (1 - a1[m, 0])\n",
    "        \n",
    "        W3 = W3 - (learning_rate * (grad_W3 / batch_size))\n",
    "        W2 = W2 - (learning_rate * (grad_W2 / batch_size))\n",
    "        W1 = W1 - (learning_rate * (grad_W1 / batch_size))\n",
    "        \n",
    "        b3 = b3 - (learning_rate * (grad_b3 / batch_size))\n",
    "        b2 = b2 - (learning_rate * (grad_b2 / batch_size))\n",
    "        b1 = b1 - (learning_rate * (grad_b1 / batch_size))\n",
    "    \n",
    "    # calculate cost average per epoch\n",
    "    cost = 0\n",
    "    for train_data in train_set[:100]:\n",
    "        a0 = train_data[0]\n",
    "        a1 = sigmoid(W1 @ a0 + b1)\n",
    "        a2 = sigmoid(W2 @ a1 + b2)\n",
    "        a3 = sigmoid(W3 @ a2 + b3)\n",
    "\n",
    "        for j in range(10):\n",
    "            cost += np.power((a3[j, 0] - train_data[1][j,  0]), 2)\n",
    "            \n",
    "    cost /= 100\n",
    "    total_costs.append(cost)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.89\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_size = [x for x in range(20)]\n",
    "plt.plot(epoch_size, total_costs)\n",
    "\n",
    "for train_data in train_set[:100]:\n",
    "    a0 = train_data[0]\n",
    "    a1 = sigmoid(W1 @ a0 + b1)\n",
    "    a2 = sigmoid(W2 @ a1 + b2)\n",
    "    a3 = sigmoid(W3 @ a2 + b3)\n",
    "    \n",
    "    predicted_number = np.where(a3 == np.amax(a3))\n",
    "    real_number = np.where(train_data[1] == np.amax(train_data[1]))\n",
    "    \n",
    "    if predicted_number == real_number:\n",
    "        number_of_correct_estimations += 1\n",
    "        \n",
    "print(f\"Accuracy: {number_of_correct_estimations / 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Vectorization\n",
    "Because of the long execution time, we've only dealt on 100 first pictures so far. We use vectorization to solve this problem. It implies that we use matrix operations instead of for loops to measure each entry of matrices separately.\n",
    "\n",
    "As a result, the processing time would be significantly reduced. The explanation for this is that matrix operations can run in parallel on multi-core CPUs. Furthermore, today's processors have instructions for working with large vector data, which will be much more effective.\n",
    "\n",
    "We have implemented the second section (Feedforward) in a vectorized way. Now we're attempting to do the same thing with backpropagation.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In vectorized notiation, we have:\n",
    "$$Cost =(\\overrightarrow{a}^{(3)} - \\overrightarrow{y})^{T}(\\overrightarrow{a}^{(3)} - \\overrightarrow{y})$$\n",
    "The activation vector at the last layer is equal to:\n",
    "$$\\overrightarrow{a}^{(3)} = \\sigma(\\overrightarrow{z}^{(3)})$$\n",
    "And $\\overrightarrow{z}^{(3)}$ is equal to:\n",
    "$$\\overrightarrow{z}^{(3)} = W^{(3)}\\overrightarrow{a}^{(2)} + \\overrightarrow{b}^{(2)}$$\n",
    "### The last layer\n",
    "##### Weight\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial W^{(3)}} = 2(\\overrightarrow{a}^{(3)} - \\overrightarrow{y})\\overrightarrow{a}^{(3)}(1 - \\overrightarrow{a}^{(3)}) \\bullet \\overrightarrow{a}^{(2)}$$\n",
    "##### Bias\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial \\overrightarrow{b}^{(3)}} = 2 (\\overrightarrow{a}^{(3)} - \\overrightarrow{y}) \\overrightarrow{a}^{(3)}(1 - \\overrightarrow{a}^{(3)})$$\n",
    "##### Activation\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial \\overrightarrow{a}^{(2)}} = {W_{3}^{T}} (2(\\overrightarrow{a}^{(3)} - \\overrightarrow{y}) \\overrightarrow{a}^{(3)}(1 - \\overrightarrow{a}^{(3)})) $$\n",
    "\n",
    "### 3rd Layer\n",
    "##### Weight\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial W^{(2)}} = \\displaystyle \\frac{\\partial Cost}{\\partial \\overrightarrow{a}^{(2)}}\\overrightarrow{a}^{(2)}(1 - \\overrightarrow{a}^{(2)}) \\bullet \\overrightarrow{a}^{(1)}$$\n",
    "##### Bias\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial \\overrightarrow{b}^{(2)}} = (\\displaystyle \\frac{\\partial Cost}{\\partial \\overrightarrow{a}^{(2)}})\\overrightarrow{a}^{(2)}(1 - \\overrightarrow{a}^{(2)})$$\n",
    "##### Activation \n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial \\overrightarrow{a}^{(1)}} = {W_{2}^{T}} (\\displaystyle \\frac{\\partial Cost}{\\partial \\overrightarrow{a}^{(2)}}) \\overrightarrow{a}^{(2)}(1 - \\overrightarrow{a}^{(2)})) $$\n",
    "\n",
    "### 2nd\n",
    "##### Weight\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial W^{(1)}} = \\displaystyle \\frac{\\partial Cost}{\\partial \\overrightarrow{a}^{(1)}}\\overrightarrow{a}^{(1)}(1 - \\overrightarrow{a}^{(1)}) \\bullet \\overrightarrow{a}^{(0)}$$\n",
    "##### Bias\n",
    "$$\\displaystyle \\frac{\\partial Cost}{\\partial \\overrightarrow{b}^{(1)}} = (\\displaystyle \\frac{\\partial Cost}{\\partial \\overrightarrow{a}^{(1)}})\\overrightarrow{a}^{(1)}(1 - \\overrightarrow{a}^{(1)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 537 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Allocate W matrix and vector b for each layer.\n",
    "\n",
    "# Initialize W with random normal distribution for each layer. \n",
    "W1 = np.random.normal(size=(16, NUMBER_OF_PIXELS))\n",
    "W2 = np.random.normal(size=(16, 16))\n",
    "W3 = np.random.normal(size=(10, 16))\n",
    "\n",
    "# Initialize b = 0, for each layer.\n",
    "b1 = np.zeros((16, 1))\n",
    "b2 = np.zeros((16, 1))\n",
    "b3 = np.zeros((10, 1))\n",
    "\n",
    "total_costs = []\n",
    "batches = [train_set[x:x+batch_size] for x in range(0, 100, batch_size)]\n",
    "for epoch in range(number_of_epochs):\n",
    "    for batch in batches:\n",
    "        # allocate grad_W matrix for each layer\n",
    "        grad_W1 = np.zeros((16, NUMBER_OF_PIXELS))\n",
    "        grad_W2 = np.zeros((16, 16))\n",
    "        grad_W3 = np.zeros((10, 16))\n",
    "        # allocate grad_b for each layer\n",
    "        grad_b1 = np.zeros((16, 1))\n",
    "        grad_b2 = np.zeros((16, 1))\n",
    "        grad_b3 = np.zeros((10, 1))\n",
    "        \n",
    "        for image, label in batch:\n",
    "            # compute the output (image is equal to a0)\n",
    "            a1 = sigmoid(W1 @ image + b1)\n",
    "            a2 = sigmoid(W2 @ a1 + b2)\n",
    "            a3 = sigmoid(W3 @ a2 + b3)\n",
    "            \n",
    "            # ---- Last layer\n",
    "            # weight\n",
    "            grad_W3 += (2 * (a3 - label) * a3 * (1 - a3)) @ np.transpose(a2)\n",
    "            \n",
    "            # bias\n",
    "            grad_b3 += 2 * (a3 - label) * a3 * (1 - a3)\n",
    "            \n",
    "            # ---- 3rd layer\n",
    "            # activation\n",
    "            delta_3 = np.zeros((16, 1))\n",
    "            delta_3 += np.transpose(W3) @ (2 *(a3 - label) * (a3 * (1 - a3)))\n",
    "            \n",
    "            # weight\n",
    "            grad_W2 += (a2 * (1 - a2) * delta_3) @ np.transpose(a1)\n",
    "            \n",
    "            # bias\n",
    "            grad_b2 += delta_3 * a2 * (1 - a2)\n",
    "                    \n",
    "            # ---- 2nd layer\n",
    "            # activation\n",
    "            delta_2 = np.zeros((16, 1))\n",
    "            delta_2 += np.transpose(W2) @ delta_3 * a2 * (1 - a2)\n",
    "            \n",
    "            # weight\n",
    "            grad_W1 += (delta_2 * a1 * (1 - a1)) @ np.transpose(image)\n",
    "                    \n",
    "            # bias\n",
    "            grad_b1 += delta_2 * a1 * (1 - a1)\n",
    "        \n",
    "        W3 = W3 - (learning_rate * (grad_W3 / batch_size))\n",
    "        W2 = W2 - (learning_rate * (grad_W2 / batch_size))\n",
    "        W1 = W1 - (learning_rate * (grad_W1 / batch_size))\n",
    "        \n",
    "        b3 = b3 - (learning_rate * (grad_b3 / batch_size))\n",
    "        b2 = b2 - (learning_rate * (grad_b2 / batch_size))\n",
    "        b1 = b1 - (learning_rate * (grad_b1 / batch_size))\n",
    "    \n",
    "    # calculate cost average per epoch\n",
    "    cost = 0\n",
    "    for train_data in train_set[:100]:\n",
    "        a0 = train_data[0]\n",
    "        a1 = sigmoid(W1 @ a0 + b1)\n",
    "        a2 = sigmoid(W2 @ a1 + b2)\n",
    "        a3 = sigmoid(W3 @ a2 + b3)\n",
    "\n",
    "        for j in range(10):\n",
    "            cost += np.power((a3[j, 0] - train_data[1][j,  0]), 2)\n",
    "            \n",
    "    cost /= 100\n",
    "    total_costs.append(cost)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.71\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_size = [x for x in range(number_of_epochs)]\n",
    "plt.plot(epoch_size, total_costs)\n",
    "number_of_correct_estimations = 0\n",
    "for train_data in train_set[:100]:\n",
    "    a0 = train_data[0]\n",
    "    a1 = sigmoid(W1 @ a0 + b1)\n",
    "    a2 = sigmoid(W2 @ a1 + b2)\n",
    "    a3 = sigmoid(W3 @ a2 + b3)\n",
    "    \n",
    "    predicted_number = np.where(a3 == np.amax(a3))\n",
    "    real_number = np.where(train_data[1] == np.amax(train_data[1]))\n",
    "    \n",
    "    if predicted_number == real_number:\n",
    "        number_of_correct_estimations += 1\n",
    "        \n",
    "print(f\"Accuracy: {number_of_correct_estimations / 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Testing the model\n",
    "We use all of our data (60k images) to learn our model now that we've optimized our implementation. Finally, we measure the model's accuracy using test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "learning_rate = 1\n",
    "number_of_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "total_costs = []\n",
    "# Initialize W with random normal distribution for each layer.\n",
    "W1 = np.random.normal(size=(16, NUMBER_OF_PIXELS))\n",
    "W2 = np.random.normal(size=(16, 16))\n",
    "W3 = np.random.normal(size=(10, 16))\n",
    "\n",
    "# Initialize b = 0, for each layer.\n",
    "b1 = np.zeros((16, 1))\n",
    "b2 = np.zeros((16, 1))\n",
    "b3 = np.zeros((10, 1))\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    np.random.shuffle(train_set)\n",
    "    batches = [train_set[x:x+batch_size] for x in range(0, len(train_set), batch_size)]\n",
    "    for batch in batches:\n",
    "        # allocate grad_W matrix for each layer\n",
    "        grad_W1 = np.zeros((16, NUMBER_OF_PIXELS))\n",
    "        grad_W2 = np.zeros((16, 16))\n",
    "        grad_W3 = np.zeros((10, 16))\n",
    "        # allocate grad_b for each layer\n",
    "        grad_b1 = np.zeros((16, 1))\n",
    "        grad_b2 = np.zeros((16, 1))\n",
    "        grad_b3 = np.zeros((10, 1))\n",
    "        \n",
    "        for image, label in batch:\n",
    "            # compute the output (image is equal to a0)\n",
    "            a1 = sigmoid(W1 @ image + b1)\n",
    "            a2 = sigmoid(W2 @ a1 + b2)\n",
    "            a3 = sigmoid(W3 @ a2 + b3)\n",
    "            \n",
    "            # ---- Last layer\n",
    "            # weight\n",
    "            grad_W3 += (2 * (a3 - label) * a3 * (1 - a3)) @ np.transpose(a2)\n",
    "            \n",
    "            # bias\n",
    "            grad_b3 += 2 * (a3 - label) * a3 * (1 - a3)\n",
    "            \n",
    "            # ---- 3rd layer\n",
    "            # activation\n",
    "            delta_3 = np.zeros((16, 1))\n",
    "            delta_3 += np.transpose(W3) @ (2 *(a3 - label) * (a3 * (1 - a3)))\n",
    "            \n",
    "            # weight\n",
    "            grad_W2 += (a2 * (1 - a2) * delta_3) @ np.transpose(a1)\n",
    "            \n",
    "            # bias\n",
    "            grad_b2 += delta_3 * a2 * (1 - a2)\n",
    "                    \n",
    "            # ---- 2nd layer\n",
    "            # activation\n",
    "            delta_2 = np.zeros((16, 1))\n",
    "            delta_2 += np.transpose(W2) @ delta_3 * a2 * (1 - a2)\n",
    "            \n",
    "            # weight\n",
    "            grad_W1 += (delta_2 * a1 * (1 - a1)) @ np.transpose(image)\n",
    "                    \n",
    "            # bias\n",
    "            grad_b1 += delta_2 * a1 * (1 - a1)\n",
    "        \n",
    "        W3 = W3 - (learning_rate * (grad_W3 / batch_size))\n",
    "        W2 = W2 - (learning_rate * (grad_W2 / batch_size))\n",
    "        W1 = W1 - (learning_rate * (grad_W1 / batch_size))\n",
    "        \n",
    "        b3 = b3 - (learning_rate * (grad_b3 / batch_size))\n",
    "        b2 = b2 - (learning_rate * (grad_b2 / batch_size))\n",
    "        b1 = b1 - (learning_rate * (grad_b1 / batch_size))\n",
    "    \n",
    "    # calculate cost average per epoch\n",
    "    cost = 0\n",
    "    for train_data in train_set:\n",
    "        a0 = train_data[0]\n",
    "        a1 = sigmoid(W1 @ a0 + b1)\n",
    "        a2 = sigmoid(W2 @ a1 + b2)\n",
    "        a3 = sigmoid(W3 @ a2 + b3)\n",
    "\n",
    "        for j in range(10):\n",
    "            cost += np.power((a3[j, 0] - train_data[1][j,  0]), 2)\n",
    "            \n",
    "    cost /= 100\n",
    "    total_costs.append(cost)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 90.01\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "epoch_size = [x for x in range(number_of_epochs)]\n",
    "plt.plot(epoch_size, total_costs)\n",
    "number_of_correct_estimations = 0\n",
    "for test_data in test_set:\n",
    "    a0 = test_data[0]\n",
    "    a1 = sigmoid(W1 @ a0 + b1)\n",
    "    a2 = sigmoid(W2 @ a1 + b2)\n",
    "    a3 = sigmoid(W3 @ a2 + b3)\n",
    "    \n",
    "    predicted_number = np.where(a3 == np.amax(a3))\n",
    "    real_number = np.where(test_data[1] == np.amax(test_data[1]))\n",
    "    \n",
    "    if predicted_number == real_number:\n",
    "        number_of_correct_estimations += 1\n",
    "        \n",
    "print(f\"Accuracy: {number_of_correct_estimations / 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Additional Steps\n",
    "    6.1 Shifting image  \n",
    "    6.2 Using another activation function  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Shifting image\n",
    "In this section, we examine the consequence of shifting input images of the test set and its effect on the accuracy of our model. To do that, we can divide this section into two steps:\n",
    "1. Training our model with full of the train set images.\n",
    "2. test our model with a shifted version of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#1\n",
    "\n",
    "total_costs = []\n",
    "# Initialize W with random normal distribution for each layer.\n",
    "W1 = np.random.normal(size=(16, NUMBER_OF_PIXELS))\n",
    "W2 = np.random.normal(size=(16, 16))\n",
    "W3 = np.random.normal(size=(10, 16))\n",
    "\n",
    "# Initialize b = 0, for each layer.\n",
    "b1 = np.zeros((16, 1))\n",
    "b2 = np.zeros((16, 1))\n",
    "b3 = np.zeros((10, 1))\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    np.random.shuffle(train_set)\n",
    "    batches = [train_set[x:x+batch_size] for x in range(0, len(train_set), batch_size)]\n",
    "    for batch in batches:\n",
    "        # allocate grad_W matrix for each layer\n",
    "        grad_W1 = np.zeros((16, NUMBER_OF_PIXELS))\n",
    "        grad_W2 = np.zeros((16, 16))\n",
    "        grad_W3 = np.zeros((10, 16))\n",
    "        # allocate grad_b for each layer\n",
    "        grad_b1 = np.zeros((16, 1))\n",
    "        grad_b2 = np.zeros((16, 1))\n",
    "        grad_b3 = np.zeros((10, 1))\n",
    "        \n",
    "        for image, label in batch:\n",
    "            # compute the output (image is equal to a0)\n",
    "            a1 = sigmoid(W1 @ image + b1)\n",
    "            a2 = sigmoid(W2 @ a1 + b2)\n",
    "            a3 = sigmoid(W3 @ a2 + b3)\n",
    "            \n",
    "            # ---- Last layer\n",
    "            # weight\n",
    "            grad_W3 += (2 * (a3 - label) * a3 * (1 - a3)) @ np.transpose(a2)\n",
    "            \n",
    "            # bias\n",
    "            grad_b3 += 2 * (a3 - label) * a3 * (1 - a3)\n",
    "            \n",
    "            # ---- 3rd layer\n",
    "            # activation\n",
    "            delta_3 = np.zeros((16, 1))\n",
    "            delta_3 += np.transpose(W3) @ (2 *(a3 - label) * (a3 * (1 - a3)))\n",
    "            \n",
    "            # weight\n",
    "            grad_W2 += (a2 * (1 - a2) * delta_3) @ np.transpose(a1)\n",
    "            \n",
    "            # bias\n",
    "            grad_b2 += delta_3 * a2 * (1 - a2)\n",
    "                    \n",
    "            # ---- 2nd layer\n",
    "            # activation\n",
    "            delta_2 = np.zeros((16, 1))\n",
    "            delta_2 += np.transpose(W2) @ delta_3 * a2 * (1 - a2)\n",
    "            \n",
    "            # weight\n",
    "            grad_W1 += (delta_2 * a1 * (1 - a1)) @ np.transpose(image)\n",
    "                    \n",
    "            # bias\n",
    "            grad_b1 += delta_2 * a1 * (1 - a1)\n",
    "        \n",
    "        W3 = W3 - (learning_rate * (grad_W3 / batch_size))\n",
    "        W2 = W2 - (learning_rate * (grad_W2 / batch_size))\n",
    "        W1 = W1 - (learning_rate * (grad_W1 / batch_size))\n",
    "        \n",
    "        b3 = b3 - (learning_rate * (grad_b3 / batch_size))\n",
    "        b2 = b2 - (learning_rate * (grad_b2 / batch_size))\n",
    "        b1 = b1 - (learning_rate * (grad_b1 / batch_size))\n",
    "    \n",
    "    # calculate cost average per epoch\n",
    "    cost = 0\n",
    "    for train_data in train_set:\n",
    "        a0 = train_data[0]\n",
    "        a1 = sigmoid(W1 @ a0 + b1)\n",
    "        a2 = sigmoid(W2 @ a1 + b2)\n",
    "        a3 = sigmoid(W3 @ a2 + b3)\n",
    "\n",
    "        for j in range(10):\n",
    "            cost += np.power((a3[j, 0] - train_data[1][j,  0]), 2)\n",
    "            \n",
    "    cost /= 100\n",
    "    total_costs.append(cost)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to the second step, we construct a function that shifts the given image 4 pixels to the right. As a result, the image's last four columns are removed, and the first four columns' pixels are replaced with black pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift_image(given_flat_image):\n",
    "    image = given_flat_image.reshape(28, 28)\n",
    "    shifted_image = np.roll(image, 4)\n",
    "    shifted_image[:, 0] = np.zeros(28)\n",
    "    shifted_image[:, 1] = np.zeros(28)\n",
    "    shifted_image[:, 2] = np.zeros(28)\n",
    "    shifted_image[:, 3] = np.zeros(28)\n",
    "    \n",
    "    return shifted_image.reshape(784, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 22.52\n"
     ]
    }
   ],
   "source": [
    "number_of_correct_estimations = 0\n",
    "for test_data in test_set:\n",
    "    a0 = shift_image(test_data[0])\n",
    "    a1 = sigmoid(W1 @ a0 + b1)\n",
    "    a2 = sigmoid(W2 @ a1 + b2)\n",
    "    a3 = sigmoid(W3 @ a2 + b3)\n",
    "    \n",
    "    predicted_number = np.where(a3 == np.amax(a3))\n",
    "    real_number = np.where(test_data[1] == np.amax(test_data[1]))\n",
    "    \n",
    "    if predicted_number == real_number:\n",
    "        number_of_correct_estimations += 1\n",
    "        \n",
    "print(f\"Accuracy: {number_of_correct_estimations / 100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the result of the above segment, the accuracy <span style=\"color:red\">has been reduced significantly</span>.\n",
    ".  \n",
    "\n",
    "**But why a 4 pixel shift to the right make it worse?** That's because the weights of the connections from all of the neurons from the first layer to a given neuron from the second layer create a pattern that, in the end, it leads to the diagnosis of which digit we're dealing with. But that only depends on the pixels that our model learns during the learning process. As a matter of fact, our model doesn't have the faintest idea of what's the meaning of each digit. It just knows what pixels on each position can lead to an expected result if they gather in one image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Using another activation function \n",
    "We have several activation functions such as Sigmoid, Tanh, ReLU, and suchlike. Every one of them has its own cons and pros based on the application. In this part, we train our model with the ReLU function instead of the sigmoid.\n",
    "![ReLU](assets/ReLU.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    result = np.array(x)\n",
    "    result[result < 0] = 0\n",
    "    return result\n",
    "\n",
    "def ReLU_derivative(x):\n",
    "    result = np.zeros((x.shape[0], 1))\n",
    "    result[x > 0] = 1\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total_costs = []\n",
    "# Initialize W with random normal distribution for each layer.\n",
    "W1 = np.random.normal(size=(16, NUMBER_OF_PIXELS))\n",
    "W2 = np.random.normal(size=(16, 16))\n",
    "W3 = np.random.normal(size=(10, 16))\n",
    "\n",
    "# Initialize b = 0, for each layer.\n",
    "b1 = np.zeros((16, 1))\n",
    "b2 = np.zeros((16, 1))\n",
    "b3 = np.zeros((10, 1))\n",
    "\n",
    "for epoch in range(number_of_epochs):\n",
    "    np.random.shuffle(train_set)\n",
    "    batches = [train_set[x:x+batch_size] for x in range(0, len(train_set), batch_size)]\n",
    "    for batch in batches:\n",
    "        # allocate grad_W matrix for each layer\n",
    "        grad_W1 = np.zeros((16, NUMBER_OF_PIXELS))\n",
    "        grad_W2 = np.zeros((16, 16))\n",
    "        grad_W3 = np.zeros((10, 16))\n",
    "        # allocate grad_b for each layer\n",
    "        grad_b1 = np.zeros((16, 1))\n",
    "        grad_b2 = np.zeros((16, 1))\n",
    "        grad_b3 = np.zeros((10, 1))\n",
    "        \n",
    "        for image, label in batch:\n",
    "            # compute the output (image is equal to a0)\n",
    "            z1 = W1 @ image + b1\n",
    "            a1 = ReLU(z1)\n",
    "            z2 = W2 @ a1 + b2\n",
    "            a2 = ReLU(z2)\n",
    "            z3 = W3 @ a2 + b3\n",
    "            a3 = ReLU(z3)\n",
    "            \n",
    "            # ---- Last layer\n",
    "            # weight\n",
    "            grad_W3 += (2 * (a3 - label) * ReLU_derivative(z3)) @ np.transpose(a2)\n",
    "                        \n",
    "            # bias\n",
    "            grad_b3 += 2 * (a3 - label) * ReLU_derivative(z3)\n",
    "            \n",
    "            # ---- 3rd layer\n",
    "            # activation\n",
    "            delta_3 = np.zeros((16, 1))\n",
    "            delta_3 += np.transpose(W3) @ (2 *(a3 - label) * ReLU_derivative(z3))\n",
    "            \n",
    "            # weight\n",
    "            grad_W2 += (ReLU_derivative(z2) * delta_3) @ np.transpose(a1)\n",
    "            \n",
    "            # bias\n",
    "            grad_b2 += delta_3 * ReLU_derivative(z2)\n",
    "                    \n",
    "            # ---- 2nd layer\n",
    "            # activation\n",
    "            delta_2 = np.zeros((16, 1))\n",
    "            delta_2 += np.transpose(W2) @ delta_3 * ReLU_derivative(z2)\n",
    "            \n",
    "            # weight\n",
    "            grad_W1 += (delta_2 * ReLU_derivative(z1)) @ np.transpose(image)\n",
    "                    \n",
    "            # bias\n",
    "            grad_b1 += delta_2 * ReLU_derivative(z1)\n",
    "        \n",
    "        W3 = W3 - (learning_rate * (grad_W3 / batch_size))\n",
    "        W2 = W2 - (learning_rate * (grad_W2 / batch_size))\n",
    "        W1 = W1 - (learning_rate * (grad_W1 / batch_size))\n",
    "        \n",
    "        b3 = b3 - (learning_rate * (grad_b3 / batch_size))\n",
    "        b2 = b2 - (learning_rate * (grad_b2 / batch_size))\n",
    "        b1 = b1 - (learning_rate * (grad_b1 / batch_size))\n",
    "    \n",
    "    # calculate cost average per epoch\n",
    "    cost = 0\n",
    "    for train_data in train_set:\n",
    "        a0 = train_data[0]\n",
    "        a1 = ReLU(W1 @ a0 + b1)\n",
    "        a2 = ReLU(W2 @ a1 + b2)\n",
    "        a3 = ReLU(W3 @ a2 + b3)\n",
    "        \n",
    "        for j in range(10):\n",
    "            cost += np.power((a3[j, 0] - train_data[1][j,  0]), 2)\n",
    "            \n",
    "    cost /= 100\n",
    "    total_costs.append(cost)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_size = [x for x in range(number_of_epochs)]\n",
    "plt.plot(epoch_size, total_costs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the diagram above. Negative values are mapped to zero in ReLU. In layer calculations, we always end up with a negative value. As a result, each perceptron's value at the last layer is always 0, and each image's cost is always 1. This is called [Dying ReLU](https://arxiv.org/abs/1903.06733)\n",
    "\n",
    "When we have millions of train data and the number of layers is too much, the ReLU function is appropriate. Since the sigmoid derivative is a number between 0 and 1, their output will be a number close to zero, and the learning process will suffer as a result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
